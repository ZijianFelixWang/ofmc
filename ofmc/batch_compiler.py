# ofmc/batch_compiler.py
import importlib
import os
import sys
import shutil
import uuid
import subprocess
import time
from pathlib import Path
from typing import Callable

import multiprocessing
import threading
from concurrent.futures import ProcessPoolExecutor, as_completed

from pypdf import PdfWriter
from tqdm import tqdm

from .parser import OFMCompiler  # å¯¼å…¥æ‚¨çš„æ ¸å¿ƒç¼–è¯‘å™¨

import colorama
from colorama import Fore, Style

from .book_builder import build_book
from .utils import extract_relevant_latex_error

import re


# +++ æ–°å¢ï¼šå…¨å±€æ‰«æä¸æ³¨å†Œå‡½æ•° +++
def scan_and_build_registry(files_to_compile: list[Path], vault_root: Path) -> dict:
    """
    [é˜¶æ®µä¸€] æ‰«ææ‰€æœ‰æ–‡ä»¶ï¼Œæ„å»ºé“¾æ¥ç›®æ ‡æ³¨å†Œè¡¨ã€‚
    """
    print(f"{Fore.CYAN}ğŸ” Pass 1/3: Scanning {len(files_to_compile)} files for link targets...{Style.RESET_ALL}")

    registry = {}
    heading_re = re.compile(r"^\s*#+\s+(.+?)\s*$")
    # åŒ¹é…è¡Œå°¾çš„ ^xxxxxxï¼Œä½†ä¹Ÿä¼šåŒ¹é…ç‹¬ç«‹ä¸€è¡Œçš„
    block_id_re = re.compile(r'\^([a-fA-F0-9]{6})\s*$')

    def create_latex_label(target_key: str) -> str:
        """ä¸ºç»™å®šçš„ç›®æ ‡å­—ç¬¦ä¸²åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„ã€å¯¹ LaTeX å®‰å…¨çš„æ ‡ç­¾ã€‚"""
        safe_str = re.sub(r'[^a-zA-Z0-9]', '', target_key)
        # ä½¿ç”¨ uuid çš„ä¸€å°éƒ¨åˆ†ç¡®ä¿å”¯ä¸€æ€§ï¼Œé˜²æ­¢ä¸åŒæ–‡ä»¶çš„ç›¸åŒæ ‡é¢˜å†²çª
        unique_hash = str(uuid.uuid5(uuid.NAMESPACE_DNS, target_key))[:8]
        return f"wikilink:{safe_str[:40]}:{unique_hash}"

    for md_path in tqdm(files_to_compile, desc="Scanning", unit="file"):
        note_name = md_path.stem

        # 1. æ³¨å†Œæ–‡ä»¶åæœ¬èº«
        registry[note_name] = create_latex_label(note_name)

        try:
            content = md_path.read_text(encoding='utf-8')
            for line in content.splitlines():
                # 2. æ³¨å†Œæ ‡é¢˜
                h_match = heading_re.match(line)
                if h_match:
                    heading_text = h_match.group(1).strip()
                    target_key = f"{note_name}#{heading_text}"
                    registry[target_key] = create_latex_label(target_key)

                # 3. æ³¨å†Œå—ID
                b_match = block_id_re.search(line)
                if b_match:
                    block_id = b_match.group(1)
                    target_key = f"{note_name}^{block_id}"
                    registry[target_key] = create_latex_label(target_key)
        except Exception as e:
            print(f"{Fore.YELLOW}Warning: Could not read or parse {md_path.name} during scan: {e}{Style.RESET_ALL}")

    print(f"{Fore.GREEN}âœ… Found and registered {len(registry)} unique link targets.{Style.RESET_ALL}")
    return registry

def logger_thread_worker(log_queue: multiprocessing.Queue, pbar: tqdm):
    """
    è¿™ä¸ªå‡½æ•°åœ¨ä¸€ä¸ªç‹¬ç«‹çš„çº¿ç¨‹ä¸­è¿è¡Œã€‚
    å®ƒçš„å”¯ä¸€å·¥ä½œå°±æ˜¯ä»å…±äº«é˜Ÿåˆ—ä¸­è·å–æ—¥å¿—æ¶ˆæ¯ï¼Œ
    å¹¶ä½¿ç”¨ pbar.write() å®‰å…¨åœ°æ‰“å°å®ƒä»¬ã€‚
    """
    while True:
        message = log_queue.get()
        if message is None:  # "None" æ˜¯æˆ‘ä»¬çº¦å®šçš„åœæ­¢ä¿¡å·
            break
        pbar.write(str(message))

def find_markdown_files(root_dir: Path, excluded_patterns: list[str]) -> list[Path]:
    """
    ä½¿ç”¨ glob æ¨¡å¼æŸ¥æ‰¾æ‰€æœ‰ Markdown æ–‡ä»¶ï¼Œå¹¶æ’é™¤åŒ¹é…æŒ‡å®šæ¨¡å¼çš„è·¯å¾„ã€‚
    è¿™æ”¯æŒç±»ä¼¼ .gitignore çš„æ¨¡å¼åŒ¹é…ã€‚

    Args:
        root_dir: Vault çš„æ ¹ç›®å½•ã€‚
        excluded_patterns: ä¸€ä¸ªåŒ…å« glob æ¨¡å¼çš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œç”¨äºæ’é™¤æ–‡ä»¶æˆ–ç›®å½•ã€‚
                           ä¾‹å¦‚: ["*.excalidraw.md", "Templates/*", "ignored_folder"]

    Returns:
        ä¸€ä¸ªç»è¿‡æ’åºå’Œè¿‡æ»¤çš„ Path å¯¹è±¡åˆ—è¡¨ã€‚
    """

    # =================================================================
    #  ROBUSTNESS CHECK: éªŒè¯è¾“å…¥æ•°æ®çš„ç±»å‹
    # =================================================================

    if not isinstance(excluded_patterns, list):
        raise TypeError(
            f"Configuration Error: 'excluded' must be a list of strings, but got {type(excluded_patterns).__name__}")

    for i, pattern in enumerate(excluded_patterns):
        if not isinstance(pattern, str):
            # æŠ›å‡ºä¸€ä¸ªéå¸¸æ˜ç¡®çš„é”™è¯¯ï¼Œå‘Šè¯‰ç”¨æˆ·å“ªé‡Œé”™äº†
            raise TypeError(
                f"Configuration Error in 'excluded' list at position {i}: \n"
                f"Expected a string pattern, but found a {type(pattern).__name__}: {pattern!r}.\n"
                f"Please ensure all items in the 'excluded' list are simple strings (e.g., \"Templates/*\")."
            )
    # =================================================================

    # 1. ä½¿ç”¨ rglob é«˜æ•ˆåœ°é€’å½’æŸ¥æ‰¾æ‰€æœ‰ .md æ–‡ä»¶ã€‚
    #    ä½¿ç”¨ sorted() ä¿è¯ç»“æœçš„é¡ºåºç¨³å®šæ€§ã€‚
    all_md_files = sorted(root_dir.rglob("*.md"))

    # å¦‚æœæ²¡æœ‰æ’é™¤è§„åˆ™ï¼Œç›´æ¥è¿”å›æ‰€æœ‰æ–‡ä»¶
    if not excluded_patterns:
        return all_md_files

    filtered_files = []
    for file_path in all_md_files:
        # 2. è·å–æ–‡ä»¶ç›¸å¯¹äº vault æ ¹ç›®å½•çš„è·¯å¾„ï¼Œç”¨äºåŒ¹é…ã€‚
        relative_path = file_path.relative_to(root_dir)

        # 3. æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦ä¸ä»»ä½•æ’é™¤æ¨¡å¼åŒ¹é…ã€‚
        #    æˆ‘ä»¬ä¸ä»…æ£€æŸ¥æ–‡ä»¶æœ¬èº«ï¼Œè¿˜æ£€æŸ¥å®ƒçš„æ‰€æœ‰çˆ¶ç›®å½•ã€‚
        #    è¿™å…è®¸åƒ "Templates" è¿™æ ·çš„æ¨¡å¼æ’é™¤æ•´ä¸ªç›®å½•ã€‚
        parts_to_check = [relative_path] + list(relative_path.parents)

        is_excluded = any(
            part.match(pattern)
            for pattern in excluded_patterns
            for part in parts_to_check
        )

        if not is_excluded:
            filtered_files.append(file_path)

    return filtered_files

def get_temp_dir() -> Path:
    """
    ä¼˜å…ˆä½¿ç”¨/dev/shmï¼Œå¦åˆ™å›é€€åˆ°å®¶ç›®å½•çš„.cacheã€‚è¿”å› Path å¯¹è±¡ã€‚
    """
    shm_path = Path('/dev/shm')
    if shm_path.exists():
        temp_root = shm_path
    else:
        temp_root = Path.home() / '.cache'

    ofmc_temp_dir = temp_root / 'ofmc_runs' / str(uuid.uuid4())
    ofmc_temp_dir.mkdir(parents=True, exist_ok=True)
    return ofmc_temp_dir


def compile_single_file_worker(md_path: Path,
                               config,
                               temp_dir: Path,
                               log_queue: multiprocessing.Queue,
                               # --- æ–°å¢å‚æ•° ---
                               is_book_mode: bool,
                               output_target_dir: Path,
                               link_registry: dict,
                               build_assets_dir: Path,
                               max_name_length: int = 18,
                               post_processors: list = None,
                               pre_processors: list = None):
    """
    åœ¨å¹¶è¡Œè¿›ç¨‹ä¸­è¿è¡Œçš„å•ä¸ªæ–‡ä»¶ç¼–è¯‘å·¥ä½œå‡½æ•°ã€‚
    - åœ¨å›¾ä¹¦æ¨¡å¼ä¸‹ï¼Œå®ƒç”Ÿæˆ .tex ç« èŠ‚æ–‡ä»¶å¹¶è¿”å›å…¶è·¯å¾„ã€‚
    - åœ¨ç‹¬ç«‹æ¨¡å¼ä¸‹ï¼Œå®ƒç”Ÿæˆ .pdf æ–‡ä»¶å¹¶è¿”å›å…¶è·¯å¾„ã€‚
    """

    def worker_logger(message: any):  # æ¥å—ä»»ä½•ç±»å‹çš„æ¶ˆæ¯
        """ä½¿ç”¨ colorama åº“å®ç°å¸¦é¢œè‰²é«˜äº®çš„æ—¥å¿—è®°å½•å™¨ã€‚"""
        prefix = f"[{md_path.stem:<{max_name_length}}]"

        # =========================================================================
        #  FIX: Explicitly convert 'message' to a string before concatenation.
        #  This handles Path objects, exceptions, and other non-string types gracefully.
        # =========================================================================
        log_line = prefix + " " + str(message)

        if 'âœ…' in log_line:
            formatted_log = f"{Fore.GREEN}{log_line}{Fore.RESET}"
        elif 'âŒ' in log_line or 'ğŸ’¥' in log_line:
            formatted_log = f"{Fore.RED}{log_line}{Fore.RESET}"
        else:
            formatted_log = log_line
        log_queue.put(formatted_log)

    try:
        # åœ¨æ¯ä¸ªè¿›ç¨‹ä¸­åˆ›å»ºç‹¬ç«‹çš„ç¼–è¯‘å™¨å®ä¾‹ï¼Œä¿è¯è¿›ç¨‹å®‰å…¨
        compiler = OFMCompiler(
            vault_root=str(config.vault_root),
            author=config.author,
            link_registry=link_registry if is_book_mode else None,
            build_assets_dir=build_assets_dir,
            post_processors=post_processors if post_processors else None,
            pre_processors=pre_processors if pre_processors else None,
        )

        # =========================================================================
        #  æ ¸å¿ƒä¿®æ”¹ï¼šæ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„å·¥ä½œæµ
        # =========================================================================
        if is_book_mode:
            # --- å›¾ä¹¦æ¨¡å¼ï¼šç”Ÿæˆ .tex ç« èŠ‚æ–‡ä»¶ ---
            worker_logger("ğŸ“– Generating TeX chapter...")

            # å‘Šè¯‰ç¼–è¯‘å™¨ç”Ÿæˆ "chapter" ç‰‡æ®µï¼Œè€Œä¸æ˜¯å®Œæ•´æ–‡æ¡£
            # **æ³¨æ„**: è¿™éœ€è¦ä½ çš„ OFMCompiler.compile æ–¹æ³•æ”¯æŒ mode å‚æ•°
            latex_content = compiler.compile(str(md_path), mode='chapter')

            # è¾“å‡ºè·¯å¾„æ˜¯å…±äº«çš„ã€éä¸´æ—¶çš„ TeX ç›®å½•
            final_tex_path = output_target_dir / md_path.with_suffix('.tex').name

            final_tex_path.write_text(latex_content, encoding='utf-8')
            worker_logger(f"âœ… TeX chapter saved: {final_tex_path.name}")

            # è¿”å›æœ€ç»ˆçš„ .tex æ–‡ä»¶è·¯å¾„
            return md_path, final_tex_path

        else:
            # --- ç‹¬ç«‹æ–‡ä»¶æ¨¡å¼ï¼šç”Ÿæˆ .pdf æ–‡ä»¶ (æ—§é€»è¾‘) ---
            worker_logger("ğŸ“„ Compiling to standalone PDF...")

            # å‘Šè¯‰ç¼–è¯‘å™¨ç”Ÿæˆå®Œæ•´çš„ "standalone" æ–‡æ¡£
            latex_content = compiler.compile(str(md_path), mode='standalone')

            # ä½¿ç”¨ä¸´æ—¶ç›®å½•æ¥å¤„ç†ä¸­é—´æ–‡ä»¶
            # æ–‡ä»¶åå¯ä»¥ç®€å•ä¸€äº›ï¼Œå› ä¸ºç›®å½•æœ¬èº«æ˜¯å”¯ä¸€çš„
            temp_tex_path = temp_dir / "document.tex"
            temp_pdf_path = temp_dir / "document.pdf"

            temp_tex_path.write_text(latex_content, encoding='utf-8')

            success = run_xelatex(temp_tex_path, temp_dir, logger=worker_logger)

            if success and temp_pdf_path.exists():
                # è¿”å›åœ¨ä¸´æ—¶ç›®å½•ä¸­çš„ pdf è·¯å¾„
                return md_path, temp_pdf_path
            else:
                worker_logger(f"âŒ PDF compilation failed for {md_path.name}")
                return md_path, None
        # =========================================================================

    except Exception as e:
        # ç»Ÿä¸€çš„é”™è¯¯å¤„ç†ï¼Œå¯¹ä¸¤ç§æ¨¡å¼éƒ½æœ‰æ•ˆ
        worker_logger(f"ğŸ’¥ CRITICAL ERROR Compiling {md_path.name}: {e}")
        import traceback
        worker_logger(traceback.format_exc())  # æ‰“å°è¯¦ç»†çš„å †æ ˆè·Ÿè¸ªä»¥å¸®åŠ©è°ƒè¯•
        return md_path, None

def merge_pdfs(pdf_paths: list[Path], output_filename: Path):
    """å°†ä¸€ç³»åˆ—PDFæ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªå¤§æ–‡ä»¶ã€‚"""
    merger = PdfWriter()
    print(f"\nMerging {len(pdf_paths)} PDFs into {output_filename}...")
    for pdf_path in tqdm(pdf_paths, desc="Merging", unit="file"):
        try:
            merger.append(str(pdf_path))
        except Exception as e:
            print(f"Warning: Could not append {pdf_path.name}. Reason: {e}")

    merger.write(str(output_filename))
    merger.close()
    print("Merge complete.")


# --- è¿™æ˜¯æœ¬æ¨¡å—çš„ä¸»å…¥å£å‡½æ•° ---
def run_batch_compilation(config):
    """æ‰¹é‡ç¼–è¯‘çš„ä¸»è°ƒåº¦å‡½æ•°ï¼Œç°åœ¨æ”¯æŒå›¾ä¹¦æ¨¡å¼å’Œç‹¬ç«‹æ–‡ä»¶æ¨¡å¼ã€‚"""
    vault_root = config.vault_root
    output_dir = config.output_dir
    excluded = config.excluded

    files_to_compile = find_markdown_files(vault_root, excluded)
    if not files_to_compile:
        print("No markdown files to compile.")
        return

    print(f"Found {len(files_to_compile)} markdown files to process.")

    ### BOOK MODE CHANGE ###
    # æ ¹æ®é…ç½®å†³å®šç¼–è¯‘æ¨¡å¼å’Œæœ€ç»ˆç›®æ ‡
    is_book_mode = config.enable_book_compile
    link_registry = {}  # åˆå§‹åŒ–ä¸ºç©ºå­—å…¸

    post_processors = config.post_processors
    pre_processors = config.pre_processors

    if is_book_mode:
        print("ğŸš€ Book Compilation Mode Enabled.")

        # +++ æ–°å¢ï¼šåœ¨ book æ¨¡å¼ä¸‹ï¼Œæ‰§è¡Œæ‰«æ +++
        link_registry = scan_and_build_registry(files_to_compile, vault_root)

        # --- ä¸´æ—¶è°ƒè¯•ä»£ç  ---
        import json
        debug_registry_path = output_dir / "debug_link_registry.json"
        with open(debug_registry_path, 'w', encoding='utf-8') as f:
            json.dump(link_registry, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“ Debug: Link registry saved to {debug_registry_path}")
        # --- ç»“æŸä¸´æ—¶ä»£ç  ---

        # åœ¨å›¾ä¹¦æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬å…ˆæŠŠæ‰€æœ‰ TeX ç« èŠ‚æ–‡ä»¶ç”Ÿæˆåˆ°ä¸€ä¸ªåœ°æ–¹
        # è¿™ä¸ªç›®å½•å°†åœ¨æœ€åè¢« book_builder ä½¿ç”¨ï¼Œæ‰€ä»¥å®ƒä¸æ˜¯ä¸´æ—¶çš„
        tex_chapters_dir = output_dir / "tex_chapters"
        tex_chapters_dir.mkdir(parents=True, exist_ok=True)
        print(f"Intermediate TeX chapters will be saved to: {tex_chapters_dir}")
    else:
        print("ğŸ“„ Individual PDF Compilation Mode.")
        individual_pdf_dir = output_dir / "individual_pdfs"
        individual_pdf_dir.mkdir(parents=True, exist_ok=True)
    ### END BOOK MODE CHANGE ###

    build_assets_dir = output_dir / "build_assets"
    os.makedirs(build_assets_dir, exist_ok=True)

    successful_compilations = []
    failed_compilations = []

    # è¿™ä¸ª map ç°åœ¨å¯ä»¥å­˜å‚¨ PDF è·¯å¾„ï¼ˆç‹¬ç«‹æ¨¡å¼ï¼‰æˆ– TeX è·¯å¾„ï¼ˆå›¾ä¹¦æ¨¡å¼ï¼‰
    compiled_output_map = {}

    manager = multiprocessing.Manager()
    log_queue = manager.Queue()
    temp_dir_root = get_temp_dir()
    max_workers = max(1, os.cpu_count() - 2)
    max_name_length = max(len(p.stem) for p in files_to_compile) if files_to_compile else 0

    try:
        with tqdm(total=len(files_to_compile), desc="Processing", unit="file") as pbar:
            log_thread = threading.Thread(target=logger_thread_worker, args=(log_queue, pbar))
            log_thread.start()

            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                futures = {}
                for md_path in files_to_compile:
                    worker_temp_dir = temp_dir_root / str(uuid.uuid4())
                    worker_temp_dir.mkdir()

                    ### BOOK MODE CHANGE ###
                    # å†³å®š worker çš„ç›®æ ‡è¾“å‡ºç›®å½•
                    # å›¾ä¹¦æ¨¡å¼ä¸‹ï¼Œworker ç›´æ¥è¾“å‡ºåˆ°å…±äº«çš„ tex_chapters_dir
                    # ç‹¬ç«‹æ¨¡å¼ä¸‹ï¼Œworker è¾“å‡ºåˆ°å®ƒè‡ªå·±çš„ä¸´æ—¶ç›®å½•
                    output_target_dir = tex_chapters_dir if is_book_mode else worker_temp_dir

                    # def compile_single_file_worker(md_path: Path,
                    #                                config,
                    #                                temp_dir: Path,
                    #                                log_queue: multiprocessing.Queue,
                    #                                # --- æ–°å¢å‚æ•° ---
                    #                                is_book_mode: bool,
                    #                                output_target_dir: Path,
                    #                                max_name_length: int = 18):

                    # æäº¤ä»»åŠ¡ï¼Œä¼ é€’æ–°çš„å‚æ•°æ¥æ§åˆ¶è¡Œä¸º
                    future = executor.submit(
                        compile_single_file_worker,
                        md_path,
                        config,
                        worker_temp_dir,
                        log_queue,
                        is_book_mode,
                        output_target_dir,
                        link_registry,
                        build_assets_dir,
                        max_name_length,
                        post_processors,
                        pre_processors
                    )
                    ### END BOOK MODE CHANGE ###
                    futures[future] = md_path

                for future in as_completed(futures):
                    original_md_path = futures[future]
                    try:
                        # worker ç°åœ¨è¿”å› (æºmdè·¯å¾„, æœ€ç»ˆè¾“å‡ºè·¯å¾„)
                        # è¾“å‡ºè·¯å¾„æ˜¯ .pdf (ç‹¬ç«‹æ¨¡å¼) æˆ– .tex (å›¾ä¹¦æ¨¡å¼)
                        md_path, result_path = future.result()

                        if result_path:
                            compiled_output_map[md_path] = result_path
                            successful_compilations.append(md_path)
                        else:
                            failed_compilations.append(md_path)
                    except Exception as e:
                        failed_compilations.append(original_md_path)
                        log_queue.put(f"[run_batch_compilation] FATAL ERROR for {original_md_path.name}: {e}")

                    pbar.set_postfix_str(f"{original_md_path.stem}", refresh=True)
                    pbar.update(1)

            log_queue.put(None)
            log_thread.join()

        # --- åå¤„ç†æ­¥éª¤ ---
        print("\nAll files processed. Starting post-compilation tasks...")

        ### BOOK MODE CHANGE ###
        if is_book_mode:
            # å›¾ä¹¦æ¨¡å¼çš„åå¤„ç†ï¼šè°ƒç”¨ book_builder
            if successful_compilations:
                # compiled_output_map åŒ…å« {md_path: tex_path} çš„æ˜ å°„
                sorter_func = load_sorter_from_file(config.sorting_script)
                #print(sorter_func)
                build_book(config, compiled_output_map, sorter_func)
            else:
                print("No chapters were successfully compiled. Skipping book generation.")
        else:
            # ç‹¬ç«‹æ–‡ä»¶æ¨¡å¼çš„åå¤„ç†ï¼šå¤åˆ¶å’Œåˆå¹¶ PDF
            print("\nCopying compiled PDFs to output directory...")
            for md_path, src_pdf_path in tqdm(compiled_output_map.items(), desc="Copying", unit="file"):
                relative_path = md_path.relative_to(vault_root)
                output_pdf_path = (individual_pdf_dir / relative_path).with_suffix(".pdf")
                output_pdf_path.parent.mkdir(parents=True, exist_ok=True)
                # åœ¨ç‹¬ç«‹æ¨¡å¼ä¸‹ï¼Œsrc_pdf_path æ˜¯åœ¨ä¸´æ—¶ç›®å½•é‡Œï¼Œéœ€è¦ shutil.move æˆ– copy
                shutil.copy(src_pdf_path, output_pdf_path)

            if config.enable_simple_merge and compiled_output_map:
                # æ³¨æ„ï¼šè¿™é‡Œçš„ compiled_output_map çš„å€¼æ˜¯ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨å¤åˆ¶åçš„è·¯å¾„
                pdfs_to_merge = [
                    (individual_pdf_dir / p.relative_to(vault_root)).with_suffix(".pdf")
                    for p in successful_compilations
                ]
                vault_name = vault_root.name
                merged_pdf_path = output_dir / f"{vault_name} - Compiled Vault.pdf"
                print(f"\nMerging {len(pdfs_to_merge)} PDFs into a single file...")
                merge_pdfs(pdfs_to_merge, str(merged_pdf_path))
        ### END BOOK MODE CHANGE ###

    except KeyboardInterrupt:
        print("\n\n================================================================================")
        print(" ğŸ›‘ BATCH COMPILATION CANCELLED BY USER")
        print("================================================================================")
        print("Shutting down worker processes gracefully... Please wait.")
        # å½“ `try` å—å› ä¸º KeyboardInterrupt é€€å‡ºæ—¶ï¼Œ`with` è¯­å¥çš„ `__exit__`
        # æ–¹æ³•ä¼šè¢«è‡ªåŠ¨è°ƒç”¨ã€‚å®ƒä¼šè°ƒç”¨ executor.shutdown(wait=True)ï¼Œ
        # è¿™ä¼šå°è¯•ç­‰å¾…å·²å¼€å§‹çš„ä»»åŠ¡å®Œæˆï¼Œä½†ä¸ä¼šå¼€å§‹æ–°ä»»åŠ¡ã€‚
        # è¿™ç§è¡Œä¸ºå·²ç»è¶³å¤Ÿâ€œä¼˜é›…â€ã€‚æˆ‘ä»¬åªéœ€æ•è·å¼‚å¸¸ï¼Œæ‰“å°æ¶ˆæ¯ï¼Œç„¶åå¹²å‡€åœ°é€€å‡ºã€‚
        sys.exit(130) # 130 æ˜¯è„šæœ¬è¢« Ctrl+C ä¸­æ–­çš„æ ‡å‡†é€€å‡ºç 

    finally:
        # --- 7. æœ€ç»ˆæ¸…ç† ---
        print("Cleaning up temporary files...")
        shutil.rmtree(temp_dir_root, ignore_errors=True)

        # =================================================================
        #  MODIFICATION 3: æ‰“å°æœ€ç»ˆçš„æ€»ç»“æŠ¥å‘Š
        # =================================================================
        print("\n" + "=" * 80)
        print(" BATCH COMPILATION SUMMARY")
        print("=" * 80)

        success_count = len(successful_compilations)
        failure_count = len(failed_compilations)

        print(f"Total files processed: {len(files_to_compile)}")
        print(f" âœ… Success: {success_count}")
        print(f" âŒ Failed:  {failure_count}")

        if failure_count > 0:
            success_rate = (success_count / (success_count + failure_count)) * 100
            print(f" ğŸ“Š Success Rate: {success_rate:.2f}%")

        if failed_compilations:
            print("\n--- Files that FAILED to compile ---")
            # æ’åºè®©è¾“å‡ºæ›´ç¨³å®šã€æ˜“äºæŸ¥çœ‹
            failed_compilations.sort()
            for md_path in failed_compilations:
                # æ‰“å°ç›¸å¯¹è·¯å¾„ï¼Œæ›´æ¸…æ™°
                print(f"  - {md_path.relative_to(vault_root)}")

        print("=" * 80)
        print("\nBatch compilation finished!")
        # =================================================================


def run_xelatex(
        tex_path: Path,
        working_dir: Path,
        logger: Callable = print,
        max_retries: int = 2,  # Initial attempt + 2 retries = 3 total
        retry_delay: float = 1.0  # Seconds to wait before retrying
):
    """
    Runs XeLaTeX with a two-pass approach and a robust retry mechanism.

    Args:
        tex_path: Path to the .tex file.
        working_dir: The directory where xelatex should run.
        logger: A logging function.
        max_retries: Number of times to retry after the first failure.
        retry_delay: Delay in seconds between retries.
    """
    command = [
        "xelatex",
        "-interaction=nonstopmode",
        "-halt-on-error",
        "-shell-escape",
        tex_path.name
    ]

    total_attempts = max_retries + 1
    last_error_output = ""

    for attempt in range(1, total_attempts + 1):
        if attempt > 1:
            logger(f"â³ Waiting {retry_delay}s before retrying... (Attempt {attempt}/{total_attempts})")
            time.sleep(retry_delay)

        logger(f"ğŸš€ Starting compilation attempt {attempt}/{total_attempts} for {tex_path.name}")

        # --- Pass 1 ---
        logger(f"  - Running XeLaTeX pass 1/2...")
        result1 = subprocess.run(
            command, cwd=working_dir, capture_output=True, text=True, encoding='utf-8'
        )

        if result1.returncode != 0:
            logger(f"  - Pass 1 failed on attempt {attempt}.")
            last_error_output = result1.stdout
            continue  # Move to the next retry attempt

        # --- Proactive Delay to prevent race condition ---
        # Give the filesystem a moment to sync the .aux file before pass 2 reads it.
        time.sleep(0.1)

        # --- Pass 2 ---
        logger(f"  - Running XeLaTeX pass 2/2...")
        result2 = subprocess.run(
            command, cwd=working_dir, capture_output=True, text=True, encoding='utf-8'
        )

        if result2.returncode == 0:
            logger(f"âœ… XeLaTeX compilation for {tex_path.name} completed successfully on attempt {attempt}.")
            return True  # Success!

        # If we are here, pass 2 failed
        logger(f"  - Pass 2 failed on attempt {attempt}.")
        last_error_output = result2.stdout
        # The loop will naturally continue to the next attempt

    # If the loop completes without returning True, all attempts have failed
    log_path = tex_path.with_suffix('.log')
    logger("\n" + "=" * 80)
    logger(f"âŒ XeLaTeX FAILED PERMANENTLY for {tex_path.name} after {total_attempts} attempts.")
    logger(f"!!! See full log for details: {log_path}")
    logger("=" * 80)
    logger("\nRelevant output from last failed attempt:\n")
    logger(extract_relevant_latex_error(last_error_output))
    logger("\nEnd of XeLaTeX output")

    return False


def load_sorter_from_file(script_path: Path):
    """åŠ¨æ€åŠ è½½ä¸€ä¸ª Python è„šæœ¬å¹¶è¿”å›å…¶ 'get_sort_key' å‡½æ•°ã€‚"""
    if not script_path or not script_path.is_file():
        return None

    try:
        spec = importlib.util.spec_from_file_location("custom_sorter", script_path)
        sorter_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(sorter_module)

        if hasattr(sorter_module, "get_sort_key"):
            return getattr(sorter_module, "get_sort_key")
        else:
            print(f"âš ï¸  Warning: Sorting script '{script_path}' does not have a 'get_sort_key' function.")
            return None
    except Exception as e:
        print(f"âš ï¸  Warning: Failed to load sorting script '{script_path}': {e}")
        return None